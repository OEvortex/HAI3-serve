"""
OpenAI API compatible Pydantic schemas - Simplified for vLLM-like serving
"""
from typing import List, Optional, Union, Dict, Any, Literal
from pydantic import BaseModel, Field
import time


# Simple function calling support
class FunctionCall(BaseModel):
    """Function call in assistant message"""
    name: str = Field(..., description="The name of the function to call")
    arguments: str = Field(..., description="The arguments to call the function with, as a JSON string")


class ToolCall(BaseModel):
    """Tool call object"""
    id: str = Field(..., description="The ID of the tool call")
    type: Literal["function"] = Field("function", description="The type of the tool call")
    function: FunctionCall = Field(..., description="The function call details")


class Function(BaseModel):
    """Function definition for tool calling"""
    name: str = Field(..., description="The name of the function to be called")
    description: Optional[str] = Field(None, description="A description of what the function does")
    parameters: Optional[Dict[str, Any]] = Field(None, description="The parameters the function accepts")


class Tool(BaseModel):
    """Tool definition"""
    type: Literal["function"] = Field("function", description="The type of the tool")
    function: Function = Field(..., description="The function definition")


class Message(BaseModel):
    """Chat message model"""
    role: Literal["system", "user", "assistant", "tool"] = Field(..., description="The role of the message author")
    content: Optional[str] = Field(None, description="The contents of the message")
    name: Optional[str] = Field(None, description="The name of the author of this message")
    tool_calls: Optional[List[ToolCall]] = Field(None, description="The tool calls generated by the model")
    tool_call_id: Optional[str] = Field(None, description="Tool call that this message is responding to")


class ChatCompletionRequest(BaseModel):
    """Chat completion request model"""
    model: str = Field(..., description="ID of the model to use")
    messages: List[Message] = Field(..., description="A list of messages comprising the conversation so far")
    max_tokens: Optional[int] = Field(None, description="The maximum number of tokens to generate", ge=1)
    temperature: Optional[float] = Field(None, description="Sampling temperature", ge=0.0, le=2.0)
    top_p: Optional[float] = Field(None, description="Nucleus sampling parameter", ge=0.0, le=1.0)
    stop: Optional[Union[str, List[str]]] = Field(None, description="Stop sequences")
    stream: Optional[bool] = Field(False, description="Whether to stream back partial progress")
    
    # Tool calling parameters
    tools: Optional[List[Tool]] = Field(None, description="A list of tools the model may call")
    tool_choice: Optional[Union[str, Dict[str, Any]]] = Field(None, description="Controls which tool is called by the model")


class CompletionRequest(BaseModel):
    """Text completion request model"""
    model: str = Field(..., description="ID of the model to use")
    prompt: str = Field(..., description="The prompt to generate completions for")
    max_tokens: Optional[int] = Field(None, description="The maximum number of tokens to generate", ge=1)
    temperature: Optional[float] = Field(None, description="Sampling temperature", ge=0.0, le=2.0)
    top_p: Optional[float] = Field(None, description="Nucleus sampling parameter", ge=0.0, le=1.0)
    stop: Optional[Union[str, List[str]]] = Field(None, description="Stop sequences")
    stream: Optional[bool] = Field(False, description="Whether to stream back partial progress")


class Usage(BaseModel):
    """Token usage statistics"""
    prompt_tokens: int = Field(..., description="Number of tokens in the prompt")
    completion_tokens: int = Field(..., description="Number of tokens in the completion")
    total_tokens: int = Field(..., description="Total number of tokens used")


class ChatChoice(BaseModel):
    """Chat completion choice"""
    index: int = Field(..., description="The index of this choice")
    message: Message = Field(..., description="The message generated by the model")
    finish_reason: Optional[Literal["stop", "length", "tool_calls"]] = Field(
        None, description="The reason the model stopped generating tokens"
    )


class CompletionChoice(BaseModel):
    """Completion choice"""
    index: int = Field(..., description="The index of this choice")
    text: str = Field(..., description="The generated text")
    finish_reason: Optional[Literal["stop", "length"]] = Field(
        None, description="The reason the model stopped generating tokens"
    )


class ChatCompletionResponse(BaseModel):
    """Chat completion response"""
    id: str = Field(..., description="A unique identifier for the completion")
    object: str = Field("chat.completion", description="The object type")
    created: int = Field(default_factory=lambda: int(time.time()), description="Unix timestamp")
    model: str = Field(..., description="The model used for completion")
    choices: List[ChatChoice] = Field(..., description="A list of completion choices")
    usage: Usage = Field(..., description="Usage statistics")


class CompletionResponse(BaseModel):
    """Completion response"""
    id: str = Field(..., description="A unique identifier for the completion")
    object: str = Field("text_completion", description="The object type")
    created: int = Field(default_factory=lambda: int(time.time()), description="Unix timestamp")
    model: str = Field(..., description="The model used for completion")
    choices: List[CompletionChoice] = Field(..., description="A list of completion choices")
    usage: Usage = Field(..., description="Usage statistics")


class ChatCompletionStreamChoice(BaseModel):
    """Streaming chat completion choice"""
    index: int = Field(..., description="The index of this choice")
    delta: Dict[str, Any] = Field(..., description="The delta message")
    finish_reason: Optional[Literal["stop", "length", "tool_calls"]] = Field(
        None, description="The reason the model stopped generating tokens"
    )


class ChatCompletionStreamResponse(BaseModel):
    """Streaming chat completion response"""
    id: str = Field(..., description="A unique identifier for the completion")
    object: str = Field("chat.completion.chunk", description="The object type")
    created: int = Field(default_factory=lambda: int(time.time()), description="Unix timestamp")
    model: str = Field(..., description="The model used for completion")
    choices: List[ChatCompletionStreamChoice] = Field(..., description="A list of completion choices")


class Model(BaseModel):
    """Model information"""
    id: str = Field(..., description="The model identifier")
    object: str = Field("model", description="The object type")
    created: int = Field(default_factory=lambda: int(time.time()), description="Unix timestamp")
    owned_by: str = Field("hai3-serve", description="The organization that owns the model")


class ModelList(BaseModel):
    """List of available models"""
    object: str = Field("list", description="The object type")
    data: List[Model] = Field(..., description="List of models")


class ErrorDetail(BaseModel):
    """Error detail"""
    message: str = Field(..., description="A human-readable error message")
    type: str = Field(..., description="The type of error")
    param: Optional[str] = Field(None, description="The parameter that caused the error")
    code: Optional[str] = Field(None, description="An error code")


class ErrorResponse(BaseModel):
    """Error response"""
    error: ErrorDetail = Field(..., description="Error details")